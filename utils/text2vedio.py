import argparse
import torch
import numpy as np
from diffusers import AutoModel, WanPipeline
from diffusers.quantizers import PipelineQuantizationConfig
from diffusers.hooks.group_offloading import apply_group_offloading
from diffusers.utils import export_to_video, load_image
from transformers import UMT5EncoderModel

import logging
import torch

class Text2Vedio:
    """
    Text2Vedio class for generating videos from text prompts using the WanPipeline.
    It loads pretrained model components and provides methods to generate and save videos.
    """

    def __init__(self):
        """
        Initialize the Text2Vedio pipeline by loading model components,
        setting device (GPU if available, else CPU), and data types.
        Records GPU device info and memory usage via logging.
        """
        # Setup logger
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(filename='text2vedio.log', filemode='a', level=logging.INFO,
                            format="%(asctime)s - %(levelname)s - %(message)s")

        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.dtype = torch.bfloat16
        self.model_name = "Wan-AI/Wan2.1-T2V-1.3B-Diffusers"

        # Log device info
        if torch.cuda.is_available():
            device_id = torch.cuda.current_device()
            device_name = torch.cuda.get_device_name(device_id)
            memory_allocated = torch.cuda.memory_allocated(device_id) / (1024 ** 2)  # MB
            memory_reserved = torch.cuda.memory_reserved(device_id) / (1024 ** 2)  # MB
            self.logger.info(f"Using GPU device {device_id}: {device_name}")
            self.logger.info(f"GPU Memory Allocated: {memory_allocated:.2f} MB")
            self.logger.info(f"GPU Memory Reserved: {memory_reserved:.2f} MB")
        else:
            self.logger.info("Using CPU device")
        
        self.text_encoder = UMT5EncoderModel.from_pretrained(self.model_name, subfolder="text_encoder", torch_dtype=torch.bfloat16)
        self.vae = AutoModel.from_pretrained(self.model_name, subfolder="vae", torch_dtype=torch.float32)
        self.transformer = AutoModel.from_pretrained(self.model_name, subfolder="transformer", torch_dtype=torch.bfloat16)
        self.pipe = WanPipeline.from_pretrained(
            self.model_name,
            vae=self.vae,
            transformer=self.transformer,
            text_encoder=self.text_encoder,
            dtype=self.dtype,
            device=self.device,
        )
    def text_to_video(self, text):
        """
        Generate video frames from a given text prompt.

        Args:
            text (str): The input text prompt for video generation.

        Returns:
            List of video frames generated by the model.
        """
        # Currently using a fixed prompt instead of the input text
        prompt = """
        The camera rushes from far to near in a low-angle shot, 
        revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in 
        for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground. 
        Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic 
        shadows and warm highlights. Medium composition, front view, low angle, with depth of field.
        """
        negative_prompt = """
        Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, 
        low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, 
        misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards
        """

        # Generate video frames using the pipeline
        output = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_frames=81,
            guidance_scale=5.0,
        ).frames
        return output

    def save_video(self, output, output_path):
        """
        Save generated video frames to a video file.

        Args:
            output (list): List of video frames to save.
            output_path (str): File path to save the video.
        """
        export_to_video(output, output_path, fps=16)
    
def main():
    """
    Main function to parse CLI arguments, run text-to-video generation,
    and save the output video.
    """
    argparser = argparse.ArgumentParser(description="Text to Video Generation")
    argparser.add_argument("--text", type=str, default="results/1000_transcription.json", help="Input text prompt for video generation")
    argparser.add_argument("--output_dir", type=str, default="results",
                           help="Directory to save the generated video file")
    args = argparser.parse_args()

    t2v = Text2Vedio()
    video_frames = t2v.text_to_video(args.text)
    filename = args.text.split('/')[-1].split('.')[0]
    output_file = f"{args.output_dir}/{filename}_video.mp4"
    t2v.save_video(video_frames, output_file)
    print(f"Video saved to {output_file}")

if __name__ == "__main__":
    main()
